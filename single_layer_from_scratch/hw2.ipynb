{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "\n",
    "#load MNIST data\n",
    "MNIST_data = h5py.File('MNISTdata.hdf5', 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:] )\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "x_test = np.float32( MNIST_data['x_test'][:] )\n",
    "y_test = np.int32( np.array( MNIST_data['y_test'][:,0] ) )\n",
    "MNIST_data.close()\n",
    "\n",
    "####################################################################################\n",
    "#Implementation of stochastic gradient descent algorithm\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "def reluDerivative(x):\n",
    "    x_new = np.copy(x)\n",
    "    x_new[x_new <= 0] = 0\n",
    "    x_new[x_new > 0] = 1\n",
    "    return x_new\n",
    "\n",
    "#number of inputs\n",
    "num_inputs = 28*28\n",
    "#number of outputs\n",
    "num_outputs = 10\n",
    "model = {}\n",
    "model['W1'] = np.random.randn(num_outputs,num_inputs) / np.sqrt(num_inputs)\n",
    "model_grads = copy.deepcopy(model)\n",
    "\n",
    "\n",
    "######################################################\n",
    "d = num_inputs\n",
    "K = num_outputs\n",
    "dH = 500\n",
    "\n",
    "W = np.random.randn(dH, d)/np.sqrt(num_inputs)\n",
    "b1 = np.random.randn(dH)\n",
    "b2 = np.random.randn(K)\n",
    "C = np.random.randn(K, dH)\n",
    "\n",
    "l_rate = 0.005\n",
    "iteration_num = 500000\n",
    "\n",
    "deriv = np.zeros(10)\n",
    "\n",
    "i = 0\n",
    "while (i < iteration_num):\n",
    "    idx = random.randint(0, len(x_train)-1)\n",
    "    x_t = x_train[idx]\n",
    "    y_t = y_train[idx]\n",
    "    \n",
    "    Z = W@x_t + b1\n",
    "    H = relu(Z)\n",
    "    U = C@H + b2\n",
    "    fun_x = softmax(U)\n",
    "    \n",
    "    for k in range(10):\n",
    "        if k == y_t:\n",
    "            deriv[k] = -(1-fun_x[k])\n",
    "        else:\n",
    "            deriv[k] = fun_x[k]\n",
    "    sigma = C.T@deriv\n",
    "    \n",
    "    C = C - l_rate*np.outer(deriv,H)\n",
    "    b2 = b2 - l_rate*deriv\n",
    "    b1 = b1 - l_rate*np.multiply(sigma,reluDerivative(H))\n",
    "    W = W - l_rate*np.outer(np.multiply(sigma,reluDerivative(H)),x_t)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "model['W'] = W\n",
    "model['b1'] = b1\n",
    "model['b2'] = b2\n",
    "model['C'] = C\n",
    "\n",
    "\n",
    "def forward(x, y, model):\n",
    "    Z = model['W']@x + model['b1']\n",
    "    H = relu(Z)\n",
    "    U = model['C']@H + model['b2']\n",
    "    return softmax(U)\n",
    "\n",
    "# #test data\n",
    "\n",
    "total_correct = 0\n",
    "\n",
    "for n in range( len(x_test)):\n",
    "    y = y_test[n]\n",
    "    x = x_test[n][:]\n",
    "    p = forward(x, y, model)\n",
    "    prediction = np.argmax(p)\n",
    "    if (prediction == y):\n",
    "        total_correct += 1\n",
    "\n",
    "print(total_correct/np.float(len(x_test) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
